import os.path
import random

configfile: "config.assembly.yml"

rule all:
  input:
    expand("outputs/sbt/{db}-{domain}-x{bfsize}-k{ksize}.sbt.zip", db=config['db'], ksize=config['db_ksizes'], domain=config['domains'], bfsize=config['bfsize']),
    expand("outputs/lca/{db}-{domain}-k{ksize}-scaled10k.lca.json.gz", db=config['db'], domain=config['domains'], ksize=config['db_ksizes'])


def sigs_in_catalog(w):
    with checkpoints.catalog.get(domain=w.domain, db=w.db).output[0].open('r') as f:
        return [l.strip() for l in f.readlines()]


rule sbt:
  output: "outputs/sbt/{db}-{domain}-x{bfsize}-k{ksize}.sbt.zip"
  input:
    catalog = "outputs/catalog/{domain}/{db}.txt",
    sigs = sigs_in_catalog
  params:
    ksize="{ksize}",
    db="{db}",
    domain="{domain}",
    bfsize="{bfsize}",
  benchmark: "benchmark/sbt/{db}-{domain}-x{bfsize}-k{ksize}.txt"
  shell: """
    sourmash index \
      -k {params.ksize} \
      -x {params.bfsize} \
      --from-file <(tail +2 {input.catalog}) \
      {output} $(head -1 {input.catalog})
    """


rule lca:
  output:
    lca = "outputs/lca/{db}-{domain}-k{ksize}-scaled10k.lca.json.gz",
    report = "outputs/lca/reports/{db}-{domain}-k{ksize}-scaled10k.txt"
  input:
    lineage = "outputs/lca/lineages/{domain}_{db}_lineage.csv",
    catalog = "outputs/catalog/{domain}/{db}.txt",
    sigs = sigs_in_catalog
  params:
    ksize="{ksize}",
    db="{db}",
    domain="{domain}",
  benchmark: "benchmark/lca/{db}-{domain}-k{ksize}.txt"
  shell: """
    sourmash lca index \
      -k {params.ksize} \
      --scaled 10000 \
      --report {output.report} \
      -f -C 3 --split-identifiers \
      --from-file <(tail +2 {input.catalog}) \
      {input.lineage} \
      {output.lca} $(head -1 {input.catalog})
  """

## Rules for building signature catalogs for each index

rule download_catalog:
  output:
    catalog="outputs/assembly_stats/{domain}/assembly_summary_{db}.txt.gz",
    generated="outputs/assembly_stats/{domain}/assembly_summary_{db}.timestamp",
  params:
    domain="{domain}",
    db="{db}"
  shell: """
    wget --header='Accept-Encoding: gzip' -O {output.catalog} https://ftp.ncbi.nlm.nih.gov/genomes/{params.db}/{params.domain}/assembly_summary.txt
    date +%Y%m%d > {output.generated}
  """

checkpoint catalog:
  output: "outputs/catalog/{domain}/{db}.txt"
  input: "outputs/assembly_stats/{domain}/assembly_summary_{db}.txt.gz"
  params:
    domain = "{domain}"
  run:
      import csv
      import gzip

      basedir = config['sig_store']

      with gzip.open(input[0], 'rt') as fp:
          fp.readline() # skip first line
          fp.read(2) # skip initial comment in header
          data = csv.DictReader(fp, delimiter='\t')

          with open(output[0], 'w') as fout:
              for row in data:
                  accession = row['assembly_accession']
                  path = f"{basedir}/{accession}.sig"
                  if not os.path.exists(path):
                      # TODO: check if we can download and compute it
                      try:
                          url = url_for_accession(accession)
                      except ValueError:
                          # TODO: log this somehow
                          print(f"can't find URL for {accession}")
                          # TODO: decide if an older version should be used
                          #       logic needs to be implemented here
                          continue

                      # if URL is valid, let's calculate locally
                      path = f"outputs/sigs/{params.domain}/{accession}.sig"

                  # either sig exists in sig_store, or can be computed
                  fout.write(path + '\n')


## Rules for computing signatures not found in config['sig_store']

def url_for_accession(accession):
    db, acc = accession.split("_")
    number, version = acc.split(".")
    number = "/".join([number[pos:pos + 3] for pos in range(0, len(number), 3)])
    url = f"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/{db}/{number}"

    from subprocess import CalledProcessError
    try:
        all_names = shell(f"curl -s -l {url}/", read=True).split('\n')
    except CalledProcessError as e:
        # TODO: might check here if it was a 404 or 5xx, assuming 404
        raise ValueError(f"Can't find URL for {accession}, tried {url}")

    full_name = None
    for name in all_names:
        db_, acc_, *_ = name.split("_")
        if db_ == db and acc == acc_:
            full_name = name
            break

    url = "https" + url[3:]
    return f"{url}/{full_name}/{full_name}_genomic.fna.gz"

    # TODO: alternative is read assembly_summary for asm_name
    #return f"{url}/{accession}_{asm_name}"


def name_for_accession(domain, accession):
    import csv
    import gzip

    if accession[:3] == "GCA":
        db = "genbank"
    else:
        db = "refseq"
    summary = f"outputs/assembly_stats/{domain}/assembly_summary_{db}.txt.gz"

    with gzip.open(summary, 'rt') as fp:
        fp.readline() # skip first line
        fp.read(2) # skip initial comment in header
        data = csv.DictReader(fp, delimiter='\t')
        for row in data:
            if row['assembly_accession'] == accession:
                name_parts = [row["assembly_accession"], " ", row['organism_name']]
                if row['infraspecific_name']:
                    name_parts += [" ", row['infraspecific_name']]
                name_parts += [', ', row['asm_name']]
                return "".join(name_parts)
        # If we get here, then the accession wasn't in the summary.
        raise ValueError("Accession not in this summary file!")


# TODO: fix the input, need to calculate genbank/refseq from accession
rule compute:
  output: "outputs/sigs/{domain}/{accession}.sig"
  input:
    genbank="outputs/assembly_stats/{domain}/assembly_summary_genbank.txt.gz",
    refseq="outputs/assembly_stats/{domain}/assembly_summary_refseq.txt.gz",
  params:
    ksizes=lambda w: ",".join(str(k) for k in config["db_ksizes"]),
    scaled=1000,
    name=lambda w: name_for_accession(w.domain, w.accession),
    url_path=lambda w: url_for_accession(w.accession),
  shell: """
    sourmash compute -k {params.ksizes} \
      --scaled {params.scaled} \
      --track-abundance \
      --name {params.name:q} \
      -o {output} \
      <(curl -s {params.url_path} | zcat)
  """

## TODO: write rule to validate SBTs
# - check that all sigs in catalog are in the SBT too

### Prepare LCA lineages

rule download_lca_scripts:
  output:
    make_lineage_csv = "scripts/make-lineage-csv.py",
    taxdump_utils = "scripts/ncbi_taxdump_utils.py",
  shell: """
    wget -qO {output.taxdump_utils} https://raw.githubusercontent.com/dib-lab/2018-ncbi-lineages/63e8dc784af092293362f2e8e671ae03d1a84d1d/ncbi_taxdump_utils.py
    chmod +x {output.taxdump_utils}
  """

rule lca_lineage_csv:
  output: "outputs/lca/lineages/{domain}_{db}_lineage.csv",
  input:
    summary = "outputs/assembly_stats/{domain}/assembly_summary_{db}.txt.gz",
    make_acc_taxid_mapping = "scripts/make-acc-taxid-mapping.py",
    make_lineage_csv = "scripts/make-lineage-csv.py",
    taxonomy = expand("outputs/taxonomy/{file}.dmp", file=('nodes', 'names')),
  shell: """
    {input.make_lineage_csv} {input.taxonomy} <({input.make_acc_taxid_mapping} {input.summary}) -o {output}
  """

rule download_taxonomy:
  output:
    names = 'outputs/taxonomy/names.dmp',
    nodes = 'outputs/taxonomy/nodes.dmp',
    generated="outputs/taxonomy/timestamp",
  shell: """
    (cd outputs/taxonomy && \
    wget https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz && \
    tar xf taxdump.tar.gz)
    date +%Y%m%d > {output.generated}
  """
